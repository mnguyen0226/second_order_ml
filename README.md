# Effectiveness of Second-Order Optimization for Non-convex Machine Learning

## About
Classical optimization algorithms have become unfavorable in the world of large models and big data. Over the past decade, first-order optimization methods such as SGD and Adam have dominated deep learning literature. Despite proven theoretical properties, second-order optimization techniques are far less prevalent and unconventional in modern deep learning due to their prohibitive computation. We bring awareness to these second-order techniques by highlighting their advantages, such as robustness to hyperparameters, finite step convergence, escaping saddle points, and resilience to adversarial effects.

## Reproducibility


## Results


## Paper
- Proposal.
- Final Paper.

## Citation
- [1] Dami Choi et al. On Empirical Comparison of Optimizers of Deep Learning. 2020. URL: https://arxiv.org/pdf/1910.05446.pdf.
- [2] Dan C. Ciresan et al. High-Performance Neural Networks for Visual Object Classification. 2011. URL: https://arxiv.org/pdf/1102.0183.pdf.
- [3] E. M. Dogo et al. “A Comparative Analysis of Gradient Descent-Based Optimization Algorithms on Convolutional Neural Networks”. In: International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS) (2018).
- [4] J. Duchi, E. Hazan, and Y. Singer. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”. In: Journal of Machine Learning Research (2011).
- [5] Gradient Descent. URL: https://en.wikipedia.org/wiki/Gradient_descent.
- [6] G. Hinton, N. Srivastava, and K. Swersky. “rmsprop: Divide the gradient by a running average of its recent magnitude”. In: (2012).
- [7] Loshchilov Ilya and Hutter Frank. Decoupled Weight Decay Regularization. 2019. URL: https://arxiv.org/pdf/1711.05101.pdf.
- [8] Sutskever Ilya et al. On the importance of initialization and momentum in deep learning. 2013. URL: https://www.cs.toronto.edu/~hinton/absps/momentum.pdf.
- [9] Diederik P. Kingma and Jimmy Lei Ba. Adam: A Method For Stochastic Optimization. 2017. URL: https://arxiv.org/pdf/1412.6980.pdf.
- [10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. “ImageNet Classification with Deep Convolutional Neural Networks”. In: Advances in neural information processing systems (2012).
- [11] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009. URL: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
- [12] F. Schneider, L. Balles, and P. Hennig. DeepOBS: A Deep Learning Optimizer Benchmark Suite. 2019. URL: https://arxiv.org/abs/1903.05499.
- [13] S. Schneider et al. Past, present and future approaches using computer vision for animal reidentification from camera trap data. 2019.
- [14] Mittal Sparsh and Vaishay Shraiysh. “A Survey of Techniques for Optimizing Deep Learning on GPUs”. In: Journal of Systems Architecture (2019).
- [15] A Wilson et al. “The marginal value of adaptive gradient methods in machine learning”. In: Advances in Neural Information Processing Systems (2017).
- [16] Peng Xu, Roosta Fred, and W. Mahoney Michael. “Newton-type methods for non-convex optimization under inexact hessian information.” In: Mathematical Programming 184.1 (2020).
- [17] Peng Xu, Roosta Fred, and W. Mahoney Michael. “Second-order optimization for non-convex machine learning: An empirical study”. In: Society for Industrial and Applied Mathematics (SIAM) (2020).
- [18] Zhewei Yao et al. ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning. 2021. URL: https://arxiv.org/abs/2006.00719.
- [19] Chengxi Ye et al. On the Importance of Consistency in Training Deep Neural Networks. 2017. URL: https://www.researchgate.net/publication/318868196_On_the_Importance_of_Consistency_in_Training_Deep_Neural_Networks
